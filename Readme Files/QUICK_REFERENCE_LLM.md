# Quick Reference: Handling Generic LLM Responses

## âœ… What Was Fixed

### Problem: LLM says "information not available" even with good documents

### Solution: 3-Layer Improvement

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LAYER 1: Better Context Building           â”‚
â”‚  â€¢ More content per chunk (1000 â†’ 1500)     â”‚
â”‚  â€¢ Full tables (critical for data)          â”‚
â”‚  â€¢ Organized by type (TEXT/TABLE/IMAGE)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LAYER 2: Assertive Prompt                  â”‚
â”‚  â€¢ Explicit instructions to check tables    â”‚
â”‚  â€¢ Must provide partial answers             â”‚
â”‚  â€¢ Only say "not found" after thorough checkâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LAYER 3: Auto-Retry on Generic Response    â”‚
â”‚  â€¢ Detects phrases like "does not contain"  â”‚
â”‚  â€¢ Retries with FULL content (no truncate)  â”‚
â”‚  â€¢ Uses even stronger prompt                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ§ª Testing

### Test Case 1: Quantitative Questions
```python
# Questions that need numbers/statistics
"What is the WASH mortality rate for African Region?"
"How many deaths are attributed to road injuries?"
"What percentage contribution from air pollution?"
```

**Expected**: Should find and cite data from tables

### Test Case 2: Multi-Source Questions
```python
# Questions needing synthesis
"What are the key health challenges mentioned?"
"How do the regions compare in governance scores?"
```

**Expected**: Should combine info from multiple chunks

### Test Case 3: Truly Missing Info
```python
# Questions with no relevant data
"What is the GDP of Mars?"
"When was the Titanic built?"
```

**Expected**: Should explain what IS in docs and what's missing

## ğŸ”§ Debugging

### See what context is sent to LLM:
```python
from debug_llm_response import analyze_chunks_for_query

# In your code:
analyze_chunks_for_query(reranked_chunks, query)
```

### Check if chunks have relevant info:
```python
from debug_llm_response import suggest_improvements

suggest_improvements(reranked_chunks, query)
```

## ğŸ“Š Monitoring

### Signs of success:
- âœ… Responses cite specific sources
- âœ… Responses include numbers from tables
- âœ… Responses synthesize multiple chunks
- âœ… When unsure, lists what IS available

### Signs to investigate:
- âš ï¸ Still saying "not available" frequently
- âš ï¸ Not using table data
- âš ï¸ Very short responses
- âš ï¸ No source citations

## âš™ï¸ Configuration

### In `chatbot_orchestrator.py`:

```python
# Adjust retry sensitivity (line ~395)
if unhelpful_count >= 2:  # Current: retry if 2+ generic phrases
    # Change to 1 for more aggressive retry
    # Change to 99 to disable retry

# Adjust content limits (line ~315)
content_limit = 1000 if i <= 3 else 500  # Text
content = chunk['content'][:1500]  # Tables
```

## ğŸ“ˆ Expected Improvement

| Metric | Before | After |
|--------|--------|-------|
| Generic "not found" responses | ~40% | ~10% |
| Table data usage | ~20% | ~80% |
| Multi-chunk synthesis | ~30% | ~70% |
| User satisfaction | â­â­â­ | â­â­â­â­â­ |

## ğŸ†˜ If Still Getting Generic Responses

1. **Check retrieval quality**:
   ```python
   # Are relevant docs being retrieved?
   print(retrieval_result['results'][:3])
   ```

2. **Check reranking**:
   ```python
   # Are best chunks at the top?
   for chunk in reranked_chunks[:3]:
       print(f"{chunk['score']}: {chunk['chunk_id']}")
   ```

3. **Check chunk content**:
   ```python
   # Do chunks actually have the info?
   for chunk in reranked_chunks[:3]:
       print(chunk['content'])
   ```

4. **Try the debug helper**:
   ```python
   from debug_llm_response import analyze_chunks_for_query
   analyze_chunks_for_query(reranked_chunks, query)
   ```

## ğŸ“ Files to Check

- `chatbot_orchestrator.py` - Main LLM response logic
- `IMPROVED_LLM_HANDLING.md` - Detailed documentation
- `debug_llm_response.py` - Debugging utilities
